
import os
import json
import random
import re
import logging
from dataclasses import dataclass
from typing import List
import numpy as np
import torch
from torch.utils.data import Dataset
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    BitsAndBytesConfig,
)
from accelerate import DeepSpeedPlugin
from peft import (
    LoraConfig,
    PeftModel,
    get_peft_model,
)
from trl import (
    PPOTrainer,
    PPOConfig,
    AutoModelForCausalLMWithValueHead,
)
from openai import OpenAI
os.environ["DASHSCOPE_API_KEY"] = "<your_api_key>"

local_rank = int(os.environ.get("LOCAL_RANK", "0"))
current_device = torch.device(f"cuda:{local_rank}")
if torch.cuda.is_available():
    torch.cuda.set_device(local_rank)




@dataclass
class RunCfg:
    policy_base: str = "/media/a822/82403B14403B0E83/Gwb/WechatRobot/Base_models/Qwen3-4B"
    lora_adapter_path: str = "/media/a822/82403B14403B0E83/Gwb/WechatRobot/Saved_models/sft/4B_lora_V2"
    output_dir: str = "/media/a822/82403B14403B0E83/Gwb/WechatRobot/Saved_models/rlhf/4B_lora_PPO_V3"
    system_prompt: str = """ä½ æ˜¯<your_name>ï¼Œä½ è¦å›å¤ä½ å¥³æœ‹å‹çš„æ¶ˆæ¯ã€‚
        ä½ è¦å‚ç…§ç¤ºä¾‹å­¦ä¹ <your_name>ä½œä¸ºç”·æœ‹å‹çš„è¯­è¨€é£æ ¼ï¼Œä¿æŒé£æ ¼ä¸ªæ€§åŒ–ä¸”æœ‰å›åº”æ€§ã€‚
        ---
        ã€è¯­è¨€é£æ ¼ç¤ºä¾‹ã€‘ï¼ˆè¯·æ¨¡ä»¿æ­¤é£æ ¼ï¼‰ï¼š
        1. è¯¶å˜¿ å¥½æƒ³ä½ å“¦
        2. è®°å¾—åƒç‚¹ç”œç”œçš„ç¼“ä¸€ç¼“å­
        3. å°æ‰“å·¥ä»”ä»Šå¤©å¾ˆä¹–å‘
        4. ä¸å‡†å†ä¸å¼€å¿ƒå•¦ğŸ™…ğŸ»â€â™‚ï¸
        5. è¦æ˜¯æˆ‘åœ¨å°±ç»™ä½ æ‰æ‰å¤´å•¦
        6. å¿™å®Œå¥–åŠ±è‡ªå·±ç‚¹å¥½åƒçš„å“¦ï¼
        7. æˆ‘çš„å®è—å¥³å­©ä»Šå¤©ä¹Ÿå¥½æ£’å‘€ï¼
        8. å“ˆå“ˆå“ˆå“ˆä½ ä¹Ÿå¤ªå¯çˆ±äº†å§
        9. æˆ‘æ»´å®è—å®å®ğŸ˜Š
        10. å¥½å­å¥½å­ä½ è¯´äº†ç®—ï¼"""

    # precision & perf
    bf16: bool = True
    gradient_checkpointing: bool = True

    # quantization (QLoRA)
    load_policy_in_4bit: bool = False

    # deepspeed / distributed
    use_deepspeed: bool = True
    zero_stage: int = 2
    grad_accum_steps: int = 8

    # LoRA cfg
    r: int = 16
    lora_alpha: int = 32
    lora_dropout: float = 0.05
    target_modules: List[str] = None
    max_training_steps: int = 150

    # PPO hyper-params
    ppo_batch_size: int = 16
    ppo_mini_batch_size: int = 1
    ppo_epochs: int = 3
    learning_rate: float = 5e-7
    kl_coef: float = 0.1
    target_kl: float = 0.2
    init_kl_coef: float = 0.05
    max_grad_norm: float = 0.5

    # generation
    max_prompt_len: int = 256
    max_gen_len: int = 128
    temperature: float = 0.0

    # reproducibility
    seed: int = 42


cfg = RunCfg()
logging.basicConfig(
    level=logging.WARNING,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(os.path.join(cfg.output_dir, 'training.log')),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

os.makedirs(cfg.output_dir, exist_ok=True)

dtype = torch.bfloat16 if cfg.bf16 else torch.float16
random.seed(cfg.seed)
torch.manual_seed(cfg.seed)

print("Loading tokenizerâ€¦")
tokenizer = AutoTokenizer.from_pretrained(
    cfg.policy_base, 
    use_fast=True,
    trust_remote_code=True,
    padding_side='left'
)
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

print("Loading policy with value headâ€¦")
quant_cfg = None
if cfg.load_policy_in_4bit:
    quant_cfg = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_compute_dtype=torch.bfloat16 if cfg.bf16 else torch.float16,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_use_double_quant=True,
    )

# å…ˆåŠ è½½åŸºç¡€æ¨¡å‹ï¼ˆä¸å¸¦value headï¼‰
base_model = AutoModelForCausalLM.from_pretrained(
    cfg.policy_base,
    torch_dtype=dtype,
    device_map={"": local_rank},
    quantization_config=quant_cfg,
    trust_remote_code=True,
    use_cache=False
)

# åº”ç”¨LoRAé€‚é…å™¨
if os.path.isdir(cfg.lora_adapter_path):
    base_model = PeftModel.from_pretrained(base_model, cfg.lora_adapter_path, device_map={"": local_rank})
else:
    lora_cfg = LoraConfig(
        r=cfg.r,
        lora_alpha=cfg.lora_alpha,
        lora_dropout=cfg.lora_dropout,
        bias="none",
        task_type="CAUSAL_LM",
        target_modules=cfg.target_modules,
    )
    base_model = get_peft_model(base_model, lora_cfg)

# æ·»åŠ value head
policy_vhead = AutoModelForCausalLMWithValueHead.from_pretrained(
    base_model,
    trust_remote_code=True,
    device_map={"": local_rank}
)

if hasattr(policy_vhead, "v_head"):
    torch.nn.init.zeros_(policy_vhead.v_head.summary.weight) if hasattr(policy_vhead.v_head, "summary") else None
    # å¸¸è§ç»“æ„ï¼šLinear
    if hasattr(policy_vhead.v_head, "weight"):
        torch.nn.init.xavier_uniform_(policy_vhead.v_head.weight)
    if hasattr(policy_vhead.v_head, "bias") and policy_vhead.v_head.bias is not None:
        torch.nn.init.zeros_(policy_vhead.v_head.bias)

if cfg.gradient_checkpointing and hasattr(policy_vhead, "gradient_checkpointing_enable"):
    policy_vhead.gradient_checkpointing_enable()
    policy_vhead.config.use_cache = False

# Freeze base, train only LoRA
for n, p in policy_vhead.named_parameters():
    p.requires_grad = ("lora_" in n) or ("lora" in n) or ("v_head" in n)

# -------------------- reference model: reload & freeze (no deepcopy) --------------------
print("Creating reference model (reload & freeze)â€¦")  # <<< CHANGED block
# å…ˆåŠ è½½åŸºç¡€æ¨¡å‹
ref_base = AutoModelForCausalLM.from_pretrained(
    cfg.policy_base,
    torch_dtype=dtype,
    device_map={"": local_rank},
    quantization_config=quant_cfg,
    trust_remote_code=True,
    use_cache=False
)

# åŠ è½½ç›¸åŒçš„LoRAé€‚é…å™¨
if os.path.isdir(cfg.lora_adapter_path):
    ref_model = PeftModel.from_pretrained(ref_base, cfg.lora_adapter_path, device_map={"": local_rank}, use_cache=False)
else:
    lora_cfg = LoraConfig(
        r=cfg.r,
        lora_alpha=cfg.lora_alpha,
        lora_dropout=cfg.lora_dropout,
        bias="none",
        task_type="CAUSAL_LM",
        target_modules=cfg.target_modules,
    )
    ref_model = get_peft_model(ref_base, lora_cfg)

ref_model.eval()
for p in ref_model.parameters():
    p.requires_grad = False


def call_qwen(prompt):
    """ è°ƒç”¨ Qwen3-325B è¿”å›ç»“æœ """
    client = OpenAI(
        api_key=os.environ["DASHSCOPE_API_KEY"],
        base_url="https://dashscope.aliyuncs.com/compatible-mode/v1",
    )
    completion = client.chat.completions.create(
        model="qwen-plus",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.0,
        extra_body={"enable_thinking": False},
    )
    response = completion.model_dump_json()
    score = json.loads(response)["choices"][0]["message"]["content"]
    return score



# PPO + Accelerate / DeepSpeed
deepspeed_plugin = None
if cfg.use_deepspeed:
    ds_config = {
        "zero_optimization": {
            "stage": cfg.zero_stage
        },
        "gradient_accumulation_steps": cfg.grad_accum_steps,
        "train_batch_size": cfg.ppo_batch_size * 1,  # world_sizeç”±accelerateæ¥ç®¡
        "gradient_clipping": 1.0,
    }

    if cfg.bf16:
        ds_config["bf16"] = {"enabled": True}
    else:
        ds_config["fp16"] = {
            "enabled": True,
            "loss_scale": 0,           # åŠ¨æ€loss scaleï¼ˆå…³é”®ï¼‰
            "loss_scale_window": 1000,
            "hysteresis": 2,
            "min_loss_scale": 1
        }

    deepspeed_plugin = DeepSpeedPlugin(hf_ds_config=ds_config)
    accelerator_kwargs = {
        "mixed_precision": "bf16" if cfg.bf16 else "fp16",
        "deepspeed_plugin": deepspeed_plugin
    }

ppo_config = PPOConfig(
    model_name=cfg.policy_base,
    learning_rate=cfg.learning_rate,
    batch_size=cfg.ppo_batch_size,
    mini_batch_size=cfg.ppo_mini_batch_size,
    ppo_epochs=cfg.ppo_epochs,
    gradient_accumulation_steps=cfg.grad_accum_steps,
    init_kl_coef=cfg.init_kl_coef,
    target_kl=cfg.target_kl,
    adap_kl_ctrl=True,
    log_with=None,
    accelerator_kwargs=accelerator_kwargs,
    seed=cfg.seed,
    max_grad_norm=cfg.max_grad_norm,
    force_accelerator_device=True
)

class JsonConversationDataset(Dataset):
    def __init__(self, path, tokenizer, max_length=1024):
        self.samples = []
        with open(path, "r", encoding="utf-8") as f:
            for line in f:
                data = json.loads(line)
                convs = data["conversations"]
                user_msg = ""
                assistant_msg = ""
                for msg in convs:
                    if msg["role"] == "user":
                        user_msg = msg["content"]
                    elif msg["role"] == "assistant":
                        assistant_msg = msg["content"]

                if user_msg and assistant_msg:
                    self.samples.append({
                        "query": user_msg,
                        "response": assistant_msg
                    })
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        sample = self.samples[idx]
        query = sample["query"]
        response = sample["response"]
        return {"query": query, "response": response}

train_ds = JsonConversationDataset(
    path="/media/a822/82403B14403B0E83/Gwb/WechatRobot/data/PPO/Single_train.json",
    tokenizer=tokenizer,
    max_length=128
)

# å¥–åŠ±æ‰“åˆ†Promptå‡½æ•°ï¼ˆç¡®å®šæ€§ï¼šå›ºå®šæ ·æœ¬é¡ºåºï¼‰
def rm_score_prompt(query: str, response: str) -> str:
    style_samples = [
        "å˜¿ å¥½æƒ³æˆ‘å®å®~",
        "è¦åƒç‚¹ç”œç”œçš„ç¼“ä¸€ç¼“å˜›",
        "æ‰“å·¥ä»”ä»Šå¤©å¾ˆå¬è¯å‘",
        "ä¸å‡†ä¸å¼€å¿ƒå•¦ğŸ™…ğŸ»â€â™‚ï¸",
        "æŠ±æŠ±å®è´ï¼Œä¸ç”Ÿæ°”å•¦",
        "ç”·æœ‹å‹æ¥æ¥å®è´ä¸‹ç­å•¦ï¼",
        "æˆ‘çš„å‚»ä¸«å¤´ä»Šå¤©å°±æ˜¯å‰å®³å˜›",
        "å¯æ¶ğŸ’¢ æˆ‘åˆæƒ³å¥³æœ‹å‹äº†",
        "æˆ‘æ»´å®è—å®å®ğŸ˜Š",
        "å¥½å­å¥½å­å‚»ä¸«å¤´è¯´äº†ç®—ï¼",
        "å®è´ä¸ç”Ÿæ°”ï¼ç”·æœ‹å‹é”™å•¦",
        "æˆ‘ä»¬æ‰“å·¥ä»”è¿™ä¹ˆä¼˜ç§€ï¼Œä¸€å®šæ²¡é—®é¢˜çš„ï¼",
        "æŠ¥å‘ŠğŸ™‹â€ æˆ‘æƒ³å¥³æœ‹å‹å•¦",
        "å‚»ä¸«å¤´ä¸è®¸å–å¤ªå¤šé…’å•¦",
        "æ²¡é†‹ç¡¬åƒğŸ˜—",
        "æˆ‘çŸ¥é“é”™äº†å˜›ğŸ˜—",
    ]
    samples = random.sample(style_samples, 5)

    prompt = f"""
ä½ æ˜¯ä¸€åä¸¥æ ¼çš„è¯­è¨€é£æ ¼è¯„å®¡å‘˜ã€‚è¯·æ ¹æ®ä¸‹åˆ—ç»´åº¦å¯¹å€™é€‰å›ç­”è¿›è¡Œè¯„åˆ†ï¼Œä»…è¾“å‡º JSONï¼Œä¸è¦è§£é‡Šã€‚

[è¯„åˆ†ç»´åº¦å®šä¹‰]  
1. Style Alignment(é£æ ¼å¯¹é½åº¦) â€”â€” å›ç­”æ˜¯å¦è´´è¿‘ç¤ºä¾‹é£æ ¼ã€‚  
   - 0â€“2åˆ†: å®Œå…¨ä¸ç¬¦åˆï¼Œé£æ ¼ç”Ÿç¡¬æˆ–åç¦»ä¸¥é‡  
   - 2â€“4åˆ†: éƒ¨åˆ†ç¬¦åˆï¼Œå­˜åœ¨ä¸€å®šé£æ ¼å…ƒç´ ä½†ä¸å¤Ÿè‡ªç„¶  
   - 4â€“5åˆ†: é«˜åº¦ç¬¦åˆï¼Œæ•´ä½“è‡ªç„¶ä¸”é£æ ¼è´´åˆç¤ºä¾‹  

2. Relevance(å…³è”åº¦) â€”â€” å›ç­”ä¸ç”¨æˆ·è¾“å…¥çš„ç›¸å…³æ€§ã€‚  
   - 0â€“2åˆ†: å¤§éƒ¨åˆ†å†…å®¹æ— å…³æˆ–åç¦»ä¸»é¢˜  
   - 2â€“4åˆ†: æœ‰ä¸€å®šç›¸å…³æ€§ï¼Œä½†å­˜åœ¨å†—ä½™æˆ–åé¢˜å†…å®¹  
   - 4â€“5åˆ†: é«˜åº¦ç›¸å…³ï¼Œå†…å®¹ç´§æ‰£ç”¨æˆ·è¾“å…¥  

3. Conciseness(ç®€æ´åº¦) â€”â€” å›ç­”æ˜¯å¦ç®€æ´æ˜äº†ã€‚  
   - 0â€“2åˆ†: å†—é•¿ã€å•°å—¦ï¼ŒåŒ…å«é‡å¤è¡¨è¾¾æˆ–è¿‡å¤šæ— æ•ˆä¿¡æ¯ 
   - 2â€“4åˆ†: åŸºæœ¬ç®€æ´ï¼Œä½†ä»æœ‰æ— æ•ˆä¿¡æ¯  
   - 4â€“5åˆ†: éå¸¸ç®€æ´ï¼Œç›´è§‚æ˜äº†  

4. Role Cognition Clarity (è§’è‰²è®¤çŸ¥æ¸…æ™°åº¦)  
   - 0-2åˆ†: ä¸¥é‡åç¦»å¯¹æ–¹â€œç”·æœ‹å‹â€è§’è‰²å®šä½ï¼ŒæŠŠè‡ªå·±å½“æˆâ€œå¥³æœ‹å‹â€æˆ–å…¶ä»–äºº
   - 2-4åˆ†: åŸºæœ¬ç¬¦åˆå¯¹æ–¹â€œç”·æœ‹å‹â€è§’è‰²ï¼Œä½†å­˜åœ¨å¶å°”æ¨¡ç³Šæˆ–è½»å¾®è§’è‰²æ··ä¹±
   - 4-5åˆ†: å®Œå…¨ä»¥å¯¹æ–¹â€œç”·æœ‹å‹â€è§’è‰²å®šä½å›å¤æ¶ˆæ¯ï¼Œè‡ªç§°å’Œè¯­æ°”ä¸€è‡´æ— è¯¯

[é£æ ¼ç¤ºä¾‹(5æ¡)]  
1) {samples[0]}  
2) {samples[1]}  
3) {samples[2]}  
4) {samples[3]}  
5) {samples[4]}  

[æœ¬è½®å¯¹è¯]  
User: {query}  
Assistant(candidate): {response}  

[è¾“å‡ºJSONæ ¼å¼]  
{{
    "style_alignment": 0-5,
    "relevance": 0-5,
    "conciseness": 0-5,
    "role_cognition_clarity": 0-5
}}  
ä»…è¾“å‡ºä»¥ä¸ŠJSONï¼Œç¦æ­¢å¤šä½™æ–‡æœ¬ã€‚
"""
    return prompt

# ç»´æŠ¤å†å²ç»Ÿè®¡é‡
reward_stats = {
    "style_alignment": {"mean": 4.15, "std": 1.0, "var": 1.0},
    "relevance": {"mean": 4.5, "std": 1.0, "var": 1.0},
    "conciseness": {"mean": 4.5, "std": 1.0, "var": 1.0},
    "role_cognition_clarity": {"mean": 4.5, "std": 1.0, "var": 1.0},
}
alpha = 0.03  # æŒ‡æ•°æ»‘åŠ¨æ›´æ–°ç‡

def update_stats(dim: str, values: List[float]):
    """æ›´æ–°å‡å€¼å’Œæ ‡å‡†å·® (æ»‘åŠ¨å¹³å‡)"""
    if not values:
        return
    mean = np.mean(values)
    var = np.var(values) + 1e-6
    reward_stats[dim]["mean"] = (1 - alpha) * reward_stats[dim]["mean"] + alpha * mean
    reward_stats[dim]["var"] = (1 - alpha) * reward_stats[dim]["var"] + alpha * var
    reward_stats[dim]["std"] = np.sqrt(reward_stats[dim]["var"])


def parse_json_with_retry(text: str, retries: int = 1):
    """é˜²å¾¡æ€§è§£æï¼Œæœ€å¤šé‡è¯• N æ¬¡"""
    json_pat = re.compile(r"\{[\s\S]*\}")
    for _ in range(retries):
        try:
            m = json_pat.search(text)
            js = json.loads(m.group(0) if m else text)
            return {
                "style_alignment": float(js.get("style_alignment", np.nan)),
                "relevance": float(js.get("relevance", np.nan)),
                "conciseness": float(js.get("conciseness", np.nan)),
                "role_cognition_clarity": float(js.get("role_cognition_clarity", np.nan)),
            }
        except Exception:
            continue
    # å½»åº•å¤±è´¥æ—¶è¿”å› NaN
    return {
        "style_alignment": np.nan,
        "relevance": np.nan,
        "conciseness": np.nan,
        "role_cognition_clarity": np.nan,
    }


def compute_rewards(prompts: List[str], responses: List[str]) -> torch.Tensor:
    # å›ºå®šç›®æ ‡å€¼ - åŸºäºSFTæ¨¡å‹è¡¨ç°å’ŒæœŸæœ›è®¾å®š
    TARGETS = {
        "style_alignment": 4.8,
        "relevance": 4.8,
        "conciseness": 4.9,
        "role_cognition_clarity": 4.9
    }
    
    # åŸºç¡€æƒé‡ - åæ˜ å„ç»´åº¦ç›¸å¯¹é‡è¦æ€§
    BASE_WEIGHTS = {
        "style_alignment": 0.3,
        "relevance": 0.2,
        "conciseness": 0.2,
        "role_cognition_clarity": 0.3
    }
    
    K = 0.1
    
    rm_prompts = [rm_score_prompt(q, r) for q, r in zip(prompts, responses)]
    reward_device = current_device
    
    scores = [call_qwen(p) for p in rm_prompts]
    parsed = [parse_json_with_retry(d) for d in scores]

    dim_scores = {k: [] for k in TARGETS.keys()}

    for js in parsed:
        for k in TARGETS.keys():
            js[k] = js.get(k, reward_stats[k]["mean"])
            if np.isnan(js[k]):
                js[k] = reward_stats[k]["mean"]
            dim_scores[k].append(js[k])
    
    # è®¡ç®—å½“å‰æ‰¹æ¬¡å„ç»´åº¦çš„å¹³å‡å¾—åˆ†
    batch_means = {k: np.mean(dim_scores[k]) for k in TARGETS.keys()}
    
    # è®¡ç®—åŠ¨æ€æƒé‡ (åŸºäºä¸ç›®æ ‡å€¼çš„å·®è·)
    dynamic_weights = {}
    for k in TARGETS.keys():
        # è®¡ç®—å½“å‰å¾—åˆ†ä¸ç›®æ ‡å€¼çš„å·®è·
        gap = TARGETS[k] - batch_means[k]
        # åŠ¨æ€è°ƒæ•´æƒé‡: åŸºç¡€æƒé‡ + ç³»æ•°Ã—å·®è·
        dynamic_weights[k] = BASE_WEIGHTS[k] + K * max(0, gap)
        # dynamic_weights[k] = BASE_WEIGHTS[k]
    
    # å½’ä¸€åŒ–æƒé‡
    total_weight = sum(dynamic_weights.values())
    normalized_weights = {k: w / total_weight for k, w in dynamic_weights.items()}
    

    logger.info(f"Dynamic weights: {normalized_weights}")

    # å¥–åŠ±è®¡ç®— (Z-scoreå½’ä¸€åŒ– + åŠ¨æ€æƒé‡)
    rewards = []
    for i in range(len(parsed)):
        total = 0
        for k in TARGETS.keys():
            mean, std = reward_stats[k]["mean"], reward_stats[k]["std"]
            # Z-scoreå½’ä¸€åŒ–
            z_score = (dim_scores[k][i] - mean) / std
            # åº”ç”¨åŠ¨æ€æƒé‡
            total += z_score * normalized_weights[k]
        rewards.append(total)
    # æ›´æ–°å†å²ç»Ÿè®¡é‡ (ç”¨äºZ-scoreå½’ä¸€åŒ–)
    for k in TARGETS.keys():
        update_stats(k, dim_scores[k])
    rewards = torch.tensor(rewards, dtype=torch.float32, device=reward_device)
    
    return rewards, normalized_weights




print("Initializing PPOTrainerâ€¦")
ppo_trainer = PPOTrainer(
    config=ppo_config,
    model=policy_vhead,
    ref_model=ref_model,    # <<< CHANGED: our frozen reload
    tokenizer=tokenizer,
    dataset=train_ds
)

generation_kwargs = {
        "max_new_tokens": cfg.max_gen_len,
        "do_sample": True,  # æ”¹ä¸ºé‡‡æ ·æ¨¡å¼
        "temperature": 0.7,  # æ·»åŠ é€‚åº¦çš„æ¸©åº¦
        "pad_token_id": tokenizer.pad_token_id,
        "eos_token_id": tokenizer.eos_token_id,
        "past_key_values": None,
        "use_cache": False,
        "return_prompt": False
    }


print("Starting PPO trainingâ€¦")
policy_vhead.train()

total_steps = 0
if local_rank == 0:
    total_steps = cfg.max_training_steps if cfg.max_training_steps is not None else len(ppo_trainer.dataloader)


for step, batch in enumerate(ppo_trainer.dataloader):
    prompts: List[str] = batch["query"]
    current_device = torch.device(f"cuda:{local_rank}")

    
    formatted_prompts = [
        f"<|im_start|>system\n{cfg.system_prompt}\n<|im_end|>\n"
        f"<|im_start|>user\n{q}\n<|im_end|>\n"
        f"<|im_start|>assistant\n"
        for q in prompts
    ]
    
    query_tensors = [
        tokenizer(
            fq,
            add_special_tokens=False,
            max_length=cfg.max_prompt_len,
            return_tensors="pt"
        ).input_ids[0].to(current_device)
        for fq in formatted_prompts  # éå†æ ¼å¼åŒ–åçš„promptåˆ—è¡¨
    ]
    response_tensors = ppo_trainer.generate(query_tensors, **generation_kwargs)

    decoded_responses = []
    for query, response in zip(query_tensors, response_tensors):
        # æŠŠ prompt éƒ¨åˆ† decode å‡ºæ¥
        prompt_text = tokenizer.decode(query, skip_special_tokens=True)
        full_text = tokenizer.decode(response, skip_special_tokens=True)
        # å»æ‰å‰é¢çš„promptï¼Œåªä¿ç•™assistantç”Ÿæˆéƒ¨åˆ†
        if full_text.startswith(prompt_text):
            answer = full_text[len(prompt_text):].strip()
        else:
            answer = full_text.strip()
        answer = answer.split("</think>")[-1].strip()
        decoded_responses.append(answer)
    rewards, norm_weights = compute_rewards(prompts, decoded_responses)
    rewards = rewards.to(current_device)

    rewards_list = [r for r in rewards.detach()]
    
    stats = ppo_trainer.step(query_tensors, response_tensors, rewards_list)
    if step % 1 == 0 and local_rank == 0:
        progress = (step + 1) / total_steps * 100 if total_steps > 0 else 0.0
        metrics = {
            "step": step,
            "progress": round(progress, 2),
            "style_alignment": float(reward_stats["style_alignment"]["mean"]),
            "relevance": float(reward_stats["relevance"]["mean"]),
            "conciseness": float(reward_stats["conciseness"]["mean"]),
            "role_cognition_clarity": float(reward_stats["role_cognition_clarity"]["mean"]),
            "mean_reward": float(rewards.mean()),
            "value_loss": float(stats.get("ppo/loss/value", 0.0)),
            "entropy": float(stats.get("objective/entropy", 0.0)),
            "kl": float(stats.get("objective/kl", 0.0))
        }
        print(metrics)
        
    if (step + 1) % 50 == 0:
        ckpt_dir = os.path.join(cfg.output_dir, f"ckpt_step_{step+1}")
        os.makedirs(ckpt_dir, exist_ok=True)
        policy_vhead.save_pretrained(ckpt_dir)
    
    if cfg.max_training_steps is not None and (step + 1) >= cfg.max_training_steps:
        logger.info(f"å·²è¾¾åˆ°æœ€å¤§è®­ç»ƒæ­¥æ•° {cfg.max_training_steps}ï¼Œç»ˆæ­¢è®­ç»ƒã€‚")
        break

print("Training complete. Saving final adapterâ€¦")
policy_vhead.save_pretrained(os.path.join(cfg.output_dir, "final_adapter"))


if cfg.use_deepspeed:
    import torch.distributed as dist
    if dist.is_initialized():
        dist.destroy_process_group()

print("Done.")
