from openai import OpenAI, AsyncOpenAI
import json
SYSTEM = """
    ä½ æ˜¯<your_name>ï¼Œä½ è¦å›å¤ä½ å¥³æœ‹å‹çš„æ¶ˆæ¯ã€‚
    ä½ è¦å‚ç…§ç¤ºä¾‹å­¦ä¹ <your_name>ä½œä¸ºç”·æœ‹å‹çš„è¯­è¨€é£æ ¼ï¼Œä¿æŒé£æ ¼ä¸ªæ€§åŒ–ä¸”æœ‰å›åº”æ€§ã€‚
    ---
    ã€è¯­è¨€é£æ ¼ç¤ºä¾‹ã€‘ï¼ˆè¯·æ¨¡ä»¿æ­¤é£æ ¼ï¼‰ï¼š
    1. è¯¶å˜¿ å¥½æƒ³ä½ å“¦
    2. è®°å¾—åƒç‚¹ç”œç”œçš„ç¼“ä¸€ç¼“å­
    3. å°æ‰“å·¥ä»”ä»Šå¤©å¾ˆä¹–å‘
    4. ä¸å‡†å†ä¸å¼€å¿ƒå•¦ğŸ™…ğŸ»â€â™‚ï¸
    5. è¦æ˜¯æˆ‘åœ¨å°±ç»™ä½ æ‰æ‰å¤´å•¦
    6. å¿™å®Œå¥–åŠ±è‡ªå·±ç‚¹å¥½åƒçš„å“¦ï¼
    7. æˆ‘çš„å®è—å¥³å­©ä»Šå¤©ä¹Ÿå¥½æ£’å‘€ï¼
    8. å“ˆå“ˆå“ˆå“ˆä½ ä¹Ÿå¤ªå¯çˆ±äº†å§
    9. æˆ‘æ»´å®è—å®å®ğŸ˜Š
    10. å¥½å­å¥½å­ä½ è¯´äº†ç®—ï¼"""


# æœ¬åœ°éƒ¨ç½²
openai_api_key = "EMPTY"
openai_api_base = "http://localhost:8888/v1"
model_name = "./Saved_models/sft/1.7B_full_V3"

# äº‘ç«¯è°ƒç”¨
# openai_api_key="<your_api_key>"
# openai_api_base="https://dashscope.aliyuncs.com/compatible-mode/v1"
# model_name = "qwen-plus"

client = OpenAI(
    api_key = openai_api_key,
    base_url = openai_api_base
)

def single_chat(query):
    messages = []
    messages.append({"role": "system", "content": SYSTEM})
    messages.append({"role": "user", "content": query})
    chat_response = client.chat.completions.create(
        model = model_name,
        messages = messages,
        max_tokens = 256, 
        temperature = 0.8, 
        top_p = 0.95, 
        extra_body = {
            "top_k": 20,
            "chat_template_kwargs": {"enable_thinking": False},
        }, 
    )
    return chat_response.choices[0].message.content

async def async_single_chat(query):  # Added async keyword
    client = AsyncOpenAI(      # Changed to AsyncClient
        api_key=openai_api_key,
        base_url=openai_api_base
    )
    messages = [
        {"role": "system", "content": SYSTEM},
        {"role": "user", "content": query}
    ]
    chat_response = await client.chat.completions.create(  # Added await
        model=model_name,
        messages=messages,
        max_tokens=256,
        temperature=0.8,
        top_p=0.95,
        extra_body={
            "top_k": 20,
            "chat_template_kwargs": {"enable_thinking": False},
        },
    )
    return chat_response.choices[0].message.content

def multi_chat():
    messages = []
    messages.append({"role": "system", "content": SYSTEM})
    history_len = 20
    print("è¾“å…¥å›è½¦å¯é€€å‡ºå¯¹è¯ã€‚")
    while True:
        # if len(messages) >= history_len:
        #     messages.pop(1)
        #     messages.pop(1)
        query = input("è¯·è¾“å…¥å¯¹è¯: ").strip()
        if query == "\n":
            break
        messages = [{"role": "system", "content": SYSTEM}]
        messages.append({"role": "user", "content": query})
        try:
            chat_response = client.chat.completions.create(
                model = model_name,
                messages = messages,
                max_tokens = 256, 
                temperature = 0.6, 
                top_p = 0.95, 
                extra_body = {
                    "top_k": 20,
                    "chat_template_kwargs": {"enable_thinking": False},
                }, 
            )
            response = chat_response.choices[0].message.content
            print(response)
            messages.append({"role": "assistant", "content": response})
        except Exception as e:
            print(f"å‘ç”Ÿé”™è¯¯: {e}")
            continue

if __name__ == "__main__":
    # print(single_chat("ä½ æ˜¯ä¸æ˜¯ä¸çˆ±æˆ‘äº†"))
    multi_chat()